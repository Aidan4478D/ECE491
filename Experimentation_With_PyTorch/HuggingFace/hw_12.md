
## HuggingFace Tutorial

### Module 1 - Transformer Models

- [Pipeline Function](#pipeline-punction)
- [Transformers](#transformers)

### Pipeline Function

- Connects model with its necessary preprocessing and postprocessing steps

```
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

Returns: `[{'label': 'POSITIVE', 'score': 0.9598047137260437}]`

- Pipeline default selects a particular pretrained model that's fine-tuned for English sentiment analysis.
- Model is downloaded and cached when you create the **classifier** object

Can choose the following pipelines:
- `feature-extraction`
- `fill-mask`
- `NER` (named entity recognition)
- `question-answering`
- `sentiment-analysis`
- `summarization`
- `text-generation`
- `translation`
- `zero-shot-classifiation`

Three main steps involved in passing text to a pipeline:
1. Text is preprocessed into a format the model can understand
2. The preprocessed inputs are passed to the model
3. The predictions of the model are post-processed so they make human readable sense

**Zero Shot Classificaiton**
- Allows you to speciy which labels to use for the classification (don't have to rely on pretrained model)
- Called zero-shot as you don't need to fine-tune the model on your data to use it

```
classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```

Returns: 
```
{
    'sequence': 'This is a course about the Transformers library',
    'labels': ['education', 'business', 'politics'],
    'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]
}
```

**Text Generation**
 - Provide prompt and model will auto-complete it by generating the remaining text
 - Kinda like predictive text feature

```
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```

Returns:
```
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows â€” data flows of various types, as seen by the '
                    'HTTP'}]
```

- Can control how many different sequences are generated (`num-return-sequences`)
- Can also control total output length (`max-length`)


**Specific Models**
- Can also identify specific models from the model hub using this [link](https://huggingface.co/models)

Ex: `generator = pipeline("text-generation", model="distilgpt2")`



**Mask Filling**
- Fill in the blanks of a given text
- I think this is what BERT was trained to do originally (as well as next sentence prediction)

```
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
```

Returns:
```
[
    {   
        'sequence': 'This course will teach you all about mathematical models.',
        'score': 0.19619831442832947,
        'token': 30412,
        'token_str': ' mathematical'
    },
    {
        'sequence': 'This course will teach you all about computational models.',
        'score': 0.04052725434303284,
        'token': 38163,
        'token_str': ' computational'
    }
]
```

- `top_k` = How many possibilities we want to display
- `<mask>` = Special mask token


**Named Entity Recognition (NER)**
- Model has to find which parts of the input text correspond to entities like persons, locations, or organizations

```
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

Returns:
```
[
    {'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
    {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
    {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

- `grouped_entities = True` tells pipeline to regroup together the parts of the sentence that correspond to the same entity
    - ex. "Hugging" and "Face" gets grouped to "Hugging Face"

**Question Answering**
- Answers questions using information from a given context

```
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

Returns: `{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}`

**Summarization**
- Reduce a text into a shorter text while keeping all (or most) important aspects referenced in the text
- Can specify a `max_length` or a `min_length` for the result
- Not gonna give an example, this one is intuitive.

**Translation**
- Can use a default model if you provide a language par in the task name (like `translation_en_to_fr`)
- Can also specify a `max_length` or a `min_length`



### Transformers

Transformer model groups (broadly):
1. GPT-like (auto-regressive transformers)
2. BERT-like (auto-encoding transformers)
3. BART/T5 -like (sequence-to-sequence transformers)

- All transformers are language models
    - Have been trained on large amounts of raw text in a self-supervised fasion
    - Training in which the objective is automatically computed from the inputs of the model
    - Humans are not needed to label the data

- These models develop statistial understanding of the language it has been trained on but they're not very useful for specific practical tasks
- General model goes thorugh a process called **transfer learning** where it's fine-tuned in a supervised way
    - Sharing trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint

- General strategy to achieve better performance is by increasing the models' sizes as well as the amount of data they're pretrained on
- Training a large model though requires a lot of data and becomes very expensive

**Pre-training** = Training a model from scratch
    - weights randomly initialized
    - training starts without any prior knowledge

**Fine-tuning** = Training a pre-trained model on a more specific dataset according to your task 
