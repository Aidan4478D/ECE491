## Textbook Chapter 14 Notes - Deep Convolutional Neural Networks

## Questions and Comments 
- I think it's interesting how they don't just zero pad the vectors but it makes sense

## The Building Blocks of CNNs
- CNNS are models that were originally inspired by how the visual cortex of the human brain works when recognizing objects
    - Development goes all the way back to the 1990s
    - Outstanding for image classification tasks
    - Convolutional architectures are often described as "feature extraction layers"

- Successfully extracting salient (relevant) features is key to performance of any machine learning algorithm
    - Traditional machine learning models rely on input features that may come from a domain expert
    - Or based on computational feature extraction techniques
- CNNs can automatically learn the features from raw data that are most useful for a particular task
    - Common to consider CNN layers as feature extractors
    - Early layers (right after input layer) extract low-level features from raw data
    - Layer layers (often fully connected layers as in a multilayer perceptron (MLP)) use these featurse to predict a continuous target value or class label
- Deep CNNs construct a "feature hierarchy" by combining the low-level features in a layer wise fashion to form high-level features
    - In images, low-level features are like edges or blobs and are captured by the earlier layers
    - These high-level features can form more complex shapes like general contours of objects like buildings, cats, or dogs

![Feature Extraction Photo](feature_extraction.png)
- Feature map = each element comes from a local patch of pixels in the input image
- Local patch of pixels = local receptive field
    - Sparse connectivity: a single element in the feature map is connected to only a small patch of pixels (different form connecting to the whole input image as in MLPs)
    - Parameter sharing: Same weights are used for different patches of the input image

- Thus using a convolution layer (instread of a fully connected MLP) significantly decreases the number of weights in the network
    - Improves the ability to capture *salient* features
    - It makes sense to assume that nearby pixels are typically more relevant to each other than pixels that are far away from each other

- Typicall CNNs are composed of several convolutional and subsampling layers followed by one or more fully connected layers at the end
    - The fully connected layers are essentially an MLP where every input i is connected to every output j with weight w(i,j)
    - subsampling layers = pooling layers
        - Do not have any learnable parameters
        - No weights or biases
    - Convolutional and fully connected layers have weights and biases that are optimized during training

Performing Discrete Convolutions
---
- Fundamental operation in a CNN
- A discrete convolution for two vectors x and w can be expressed as: y = w * x
    - x is input or signal
    - w is the filter or kernel

![Discrete Convolution Formula](discrete_conv.png)
- Use the -inf and inf to represent all of the values
    - Assume that x and w are filled with zeros
- The output vector y is of finite size

- Computing the sum with one index going in the reverse direction = sum with both indices in the forward direction
    - Flip and slide!!! (They even kinda mention it "This operation is repeated like in a sliding window approach to get all the output elements")
    - Then can simply compute the dot product


![Flip and Slide](flip_and_slide.png)
- Rotated filter w' is getting shifted by two cells every shift
- Shift is a hyperparameter of a convolution called **stride**
    - Has to be a positive number < size of input vector

- So far, only used zero-padding to compute finite sized output vectors in convolutions
    - Padding can technically be applied with any number >= 0
    - Boundary cells may be treated differently than the cells located in the middle of x
        - As toward the center, there will be more computations involved than around the boundaries
- Three modes of padding that are commonly used in practice
    - Full: Padding parameter is set to p = m (window size) - 1 (increases dimensions of output, thus rarely used in CNNs)
    - Same: Used to ensure that the output vector has the same size as the input vector x. Padding parameter is computed according to filter size as well as requirement that input and output have the same size.
    - Valid: Padding where p = 0 (no padding)
- Most commonly used padding in CNNs is same padding
    - Same padding preserves the size of the vector (height and width of the input images)
    - Makes desigining a network architecture more convenient
- Valid padding disadvantage is that the volume of tensors will decrease substantially in NNs with many layers
    - Can be detrimental to the network's performance
    - Should preserve the spatial size using same padding for the convolutional layers 
    - Should decrease the spatial size via pooling layers or convolutional layers with stride 2
- Full padding size results in an output larger than the input
    - Usually used in signal processing where it's important to minimize boundary effects
    - In a deep learning approach, boundary effects are usually not an issue

- The output size of a convolution ~ number of times that the filter is shifted along the input vector
    - Input vector has size `n` and the filter is of size `n`. With padding `p` and stride `s`, the size of the output is: `o = floor((n + 2p - m) / s) + 1`

```python3
import numpy as np
def conv1d(x, w, p=0, s=1):
    w_rot = np.array(w[::-1])
    x_padded = np.array(x)
    if p > 0:
        zero_pad = np.zeros(shape=p)
        x_padded = np.concatenate([
            zero_pad, x_padded, zero_pad
        ])
    res = []
    for i in range(0, int((len(x_padded) - len(w_rot))) + 1, s):
        res.append(np.sum(x_padded[i:i+w_rot.shape[0]] * w_rot))
    return np.array(res)
```
- "naive" implementation to compute 1d convolutions

Discrete Convolutions in 2D
---
- Pretty much the exact same thing as in 1D but now a double sum lol
- Also now X and W are matrices
- All techniques like zero padding, rotating the filter matrix, and the use of strides are also applicable to 2d convolutions
    - Provided they are extended to both dimensions independently

![2D Discrete Convolution](2d_discrete_conv.png)

![2D Discrete Convolution Example](2d_conv_example.png)

```python3
import numpy as np
import scipy.signal

def conv2d(X, W, p=(0, 0), s=(1, 1)):
    W_rot = np.array(W)[::-1,::-1]
    X_orig = np.array(X)
    n1 = X_orig.shape[0] + 2*p[0]
    n2 = X_orig.shape[1] + 2*p[1]
    X_padded = np.zeros(shape=(n1, n2))
    X_padded[p[0]:p[0]+X_orig.shape[0], p[1]:p[1]+X_orig.shape[1]] = X_orig 

    res = []
    for i in range(0, int((X_padded.shape[0] - W_rot.shape[0])/s[0])+1, s[0]):
        res.append([])
        for j in range(0, int((X_padded.shape[1] - W_rot.shape[1])/s[1])+1, s[1]):
            X_sub = X_padded[i:i+W_rot.shape[0], j:j+W_rot.shape[1]]
            res[-1].append(np.sum(X_sub * W_rot))

    return(np.array(res))
```
- Again a "naive" implementation for the purpose of understanding
- There's more efficent implementations that use the FFT to compute convolutions
    - In the context of NNs, the size of a convolution kernel is much smaller than the size of the input image


Subsampling Layers
---
- Typically applied in two forms of pooling operations in CNNs: Max-pooling and mean-pooling
    - Typically denoted by P(n1 x n2) (subscripted)
    - Subscript determines the size of the neighborhood (number of adjacent pixels in each dimension) where the max or mean operation is performed
    - Refer to this neighborhood as the **pooling size**

![CNN Pooling Types](cnn_pooling_types.png)
- Max pooling = max value from neighborhood of pixels
- Mean pooling = average value in the whole neighborhood

Advantage of max pooling:
    - Introduces a local invariance; Small changes in a local neighborhood do not change the result of max pooling
    - Thus it helps with generating robust features immune to noise
    - Pooling decreases the size of features which results in higher computational efficiency
        - Might help reduce the degree of overfitting as well

- Some CNN architectures don't have pooling
    - Instead they use convolutional layers with a stride of 2
    - Can think of this as a pooling layer with learnable weights
- In a traditional NN, the most important operation is matrix multiplication
- In a CNN, the most important operation is the convolution `Z = W * X + b`
    - X is a matrix represent pixels in a *height x width* arrangement

Implementing a CNN
---
- Input to a convolutional layer may contain one or more 2D arrays or matrices with dimensions N1 x N2
    - Matrices are called channels
- Conventional implementaitons expect a rank-3 tensor representation as input (extra dimension is number of input channels)
    - Ex. if image is colored and used the RGB color mode, then the third dimension C(in) = 3 for each R, G, B
    - Ex. in grayscale, C(in) = 1

- When working with images, can read them into NumPy arrays using the uint8 datarype which takes values from [0, 255]
    - Sufficient to store the pixel information in RGB images
- To incorporate multiple input channels in the convolution, perform convolution for each channel seperately and the nadd results together using matrix summation

![CNN Implementaiton Example](cnn_implementation_example.png)
- There are three input channels (thus kernel tensor is 4D)
- Each matrix is m1 x m2 and there are three of them, one for each input channel
- There are five such kernels accountinf for five output feature maps
- There is a pooling layer for subsampling the feature maps
