## Textbook Chapter 12 Notes and Questions

## Questions
- Tensor vs NumPy array

- Python is limited to execution on one core due to the global interpreter lock (GIL)
- GPUs can be used to optimize weight parameters quickly
    - Challenge is writing code to target GPUs is not simple

- PyTorch = scalable and multiplatform programming interface for implementing and running ML alrogithms including wrappers for DL
- Primarily developed by the researchers and engineers from Facebook AI Research
- ALlows execution on CPUs, GPUs, and other devices like TPUs

- PyTorch is built around a computation graph composed of a set of nodes
    - Each node represents an operation that have 0+ inputs
    - Evaluates operations, executes computation, and returns concrete values immediately
    - Computation graph in PyTorch is defined implicitly rather than constructed in advance and executed after

- Tensors can be understood as a generalization of scalars, vectors, matrices, and so on.
    - Scalar = rank-0 tensor
    - Vector = rank-1 tensor
    - Matrix = rank-2 tensor
    - Stacked Matrices = rank-3 tensor

    - Similar to NumPy arrays except tensors are optimized for automatic differentiation and can run on GPUs

Creating Tensors in PyTorch
---
```python
import torch
import numpy as np

a = [1,2,3]
b = np.array([4,5,6], dtype=np.int32)

t_a = torch.tensor(a)
t_b = torch.from_numpy(b)

# create a tensor with random values
rand_tensor = torch.rand(2,3)

# manipulating data type
t_a_new = t_a.to(torch.int64)

# transpose a tensor
t_b_tr = torch.transpose(t_b, 0, 1)

# reshape a tensor
t = torch.zeros(30)
t_reshape = t.reshape(5, 6)

# remove unncessary dimensions
t = torch.zeros(1, 2, 1, 4, 1)
t_sqz = torch.squeeze(t, 2)
```

Applying Mathematical Operations to Tensors
---
```python
t1 = 2 * torch.rand(5, 2) - 1
t2 = torch.normal(mean=0, std=1, size=(5, 2))

# element-wies multiplication
t3 = torch.multiply(t1, t2)

# other mathematical operaions
# specify what axis to compute these oprations on with axis
t4 = torch.mean(t1, axis=0)
t4 = torch.sum(t1, axis=0)
t4 = torch.std(t1, axis=0)

# matrix multiplication
t5 = torch.matmul(t1, torch.transpose(t2, 0, 1))

# compute the L^p norm of a tensor
norm_t1 = torch.linalg.norm(t1, ord=2, dim=1)
```

Split, stack, and concatenate tensors
---
```python
# create a tensor with a list of 6 values
t = torch.rand(6)

==> tensor([0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999])

# split tensor into three lists of two elements
# if the tensor size is not divisible by the chunks value, the last chunk will be smaller
t_splits = torch.chunk(t, 3)

==> [array([0.758, 0.279], dtype=float32),
     array([0.403, 0.735], dtype=float32),
     array([0.029, 0.8 ], dtype=float32)]

# concatenate tensors
A = torch.ones(3)
B = torch.zeros(2)
C = torch.cat([A, B], axis=0)

==> tensor([1., 1., 1., 0., 0.])
```

Building input pipelines in PyTorch
---
- Usually train the model incrementally using an interative optimzation algorithm like SGD
- `torch.nn` is a module for building NN modles
- Typically need to load data from disk in chunks (i.e. batch by batch)
    - "batch" = "minibatch"

PyTorch DataLoader from Existing Tensors
---
- If data exists as a tensor object, python list, or NumPy array, we cna create a dataset loader
    - Returns an object of the `DataLoader` class
    - Can be used to iterate through the individual elements in the input dataset

```python
from torch.utils.data import DataLoader
t = torch.arange(6, dtype=torch.float32)
data_loader = DataLoader(t)

==>

tensor([0.])
tensor([1.])
tensor([2.])
tensor([3.])
tensor([4.])
tensor([5.])

# creating batches from this dataset
data_loader = DataLoader(t, batch_size=3, drop_last=False)

==>

batch 1: tensor([0., 1., 2.])
batch 2: tensor([3., 4., 5.])
```

Combining Tensors into a joint dataset
- Could have a tensor for features and one for labels

```python
t_x = torch.rand([4, 3], dtype=torch.float32)
t_y = torch.arange(4)

from torch.utils.data import Dataset
class JointDataset(Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __len__(self):
        return len(self.x)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

joint_dataset = JointDataset(t_x, t_y)

==>

x: tensor([0.7576, 0.2793, 0.4031]) y: tensor(0)
```

A custom `Dataset` class must contain the following methods to be used by the data loader later on:
    - __init__(): Where the initial logic happens like reading existing arrays, loading a file, filtering data, etc.
    - __getitem__(): Returns the corresponding sample to the given index

- Common source of error could be that the element-wise correspondence between the initial features and the labels might be lost (if two datasets are shuffeled seperately)

Shuffle, batch, and repeat
---
- When doing SGD, it's important to feed training data as randomly shuffeled batches

```python
# shuffled version data loader
# each batch contains two data records (x) and corresponding labels (y)
data_loader = DataLoader(dataset=joint_dataset, batch_size=2, shuffle=True)
```
- Rows are shuffled without losing the one-to-one correspondence between the entries in x and y
- When training for multiple epochs, need to shuffle and iterate over dataset by the desired number of epochs
- Elements in each batch should also be shuffeled per iteration



