## Textbook Chapter 13 Notes - The Mechanics of PyTorch

## Questions and Comments 
- Glorot initialization is cool

## Notes
- Will use different aspects of PyTorch's API to implement NNs
- Will use `torch.nn` module to provide multpiple layers of abstraction to make the implementation of standard architectures convenient

**Key features of PyTorch**
- Uses dynamic computational graphs
- Are more flexible compared to static graphs
- These graphs are debugging friendly: PyTorch allows for interleaving the graph declaration and graph evaluation steps
- Can execute the code line by line while having full access to all variables
- PyTorch is funded and supported by Facebook

PyTorch Computation Graphs
---
- Its compuations based on a *directed acyclic graph* (DAG) (lol H5G throwback)
- Relies on building a computation graph at its core
    - Uses this graph to derive relationships between tensors from the input to the output
    - Simply a network of nodes where each node represents an operation

Create a simple graph representing z = 2 * (a - b) + c
```python
import torch
def compute_z(a, b, c):
    r1 = torch.sub(a, b)
    r2 = torch.mul(r1, 2)
    z = torch.add(r2, c)
    return z
```
- Now can simply cally this function with tensor objects as function arguments

Create a tensor to allow gradients to be computed:
```python3
a = torch.tensor(3.14, requires_grad=True)

# can also just use the requires_grad_() function
w = torch.tensor([1.0, 2.0, 3.0])
w.requires_grad_()

```
- As of 2021, only tensors of floating point and complex type can require gradients
- Set to false by default

- Initialzing model parameters with random weights is necessary to break the symmetry during backpropagation
    - Otherwise a multilayer NN would be no more useful than a single-layer NN like logistic regression
    - When creating a PyTorch tensor, can also use a random initialization scheme
- PyTorch can generate random numbers based aon a lot of probability distributions
```python3
# initialize using Glorot initialization
import torch.nn as nn

torch.manual_seed(1)
w.requires_grad_()
nn.init.xavier_normal_(w)
```
- It was often observed than random uniform or random normal weight initialization could result in poor model performance during training
- Now roughly balance the variance of the gradients across different layers
    - Otherwise some layers may get too much attention during training while others lag behind

Computing Gradients
---
- Optimizing NNs requires computing gradients of the loss wrt the NN weights
    - Required for algorithms like SGD
- Can also use gradients to find out why a NN model is making a particular prediction for a test sample
- PyTorch provides a context for calculating gradients of tensors wrt its dependent nodes in a computation graph
    - Can call the `backward` method from the `torch.autograd` module
    - Computes the sum of gradients of the given tenstor with regard to the terminal nodes in the graph

- Automatic Differentiation = like an implementation of the chain rule for computing gradients of nested functions
    - Set of computational techniques for computing gradients of arbitrary arithmetic operations
    - Gradients of a computation are obtained by accumulating the gradients through repeated applications of the chain rule
    - Uses reverse accumulation (dy/dx0 = dy/dx1 * dx1/dx0)

- Adversarial example = add some small imperceptible noise to the input which can result in misclassification


Simplifying Implementations of Common Architectures
---
- Use `nn.Sequential`
    - The layers stored inside the model are connected in a cascaded way
```python3
model = nn.Sequential(
    nn.Linear(4, 16),
    nn.ReLU(),
    nn.Linear(16, 32),
    nn.ReLU()
)
model

==>

Sequential(
    (0): Linear(in_features=4, out_features=16, bias=True)
    (1): ReLU()
    (2): Linear(in_features=16, out_features=32, bias=True)
    (3): ReLU()
)
```
- Specified the layers and instantiated the `model` after passing the layers to the `nn.Sequential` class
- Output of the first fully connected layer is the input to the first ReLU layer
- Output of the first ReLU layer is the input for the second fully connected layer
- Output of the second ReLU fully connected layer is the input to the second ReLU

- Can experiment with:
    - Choosing different activation functions
    - Initializing layer paramyers with `nn.init`
    - Applying regularization terms to the layer parameters to prevent overfitting (L1 or L2)

```
nn.init.xavier_uniform_(model[0].weight)
l1_weight = 0.01
l1_penalty = l1_weight * model[2].weight.abs().sum()
```
- Can specify which type of optimizer loss function you want
    - For optimizers, SGD and Adam are the most widely used methods
    - Choice depends on the task (ex. use MSE for a regression problem)
    - Can use precision, recall, accuracy, and AUC to evaluate models
```python3
loss_fn = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
```

- XOR classification problem analyzes capacity of a model with regard to capturing the nonlinear decision boundary between two classes
    - A simple model with no hidden layer can only derive a linear decision boundary ==> unable to solve XOR problem
    - Need to add one+ hidden layers connected via nonlinear activation functions
    - Thus can add a hidden layer and compare different numbers of hidden units until satisfactory results are achieved

- General rule of thumb: More layers, the more neurons, and the larger the capcity of the model will be
    - Capcity can be thought of how readuly the model can approximate complex functions
    - Larger models have more parameters but are usually harder to train (prone to overfitting)
    - Made network deeper than wider as fewer parameters are required to achieve a comparable model capacity
        - However this makes model prone to the vanishing / exploding gradient problem
