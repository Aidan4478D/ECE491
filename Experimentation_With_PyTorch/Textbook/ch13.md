## Textbook Chapter 13 Notes - The Mechanics of PyTorch

## Questions and Comments 
- Glorot initialization is cool

## Notes
- Will use different aspects of PyTorch's API to implement NNs
- Will use `torch.nn` module to provide multpiple layers of abstraction to make the implementation of standard architectures convenient

**Key features of PyTorch**
- Uses dynamic computational graphs
- Are more flexible compared to static graphs
- These graphs are debugging friendly: PyTorch allows for interleaving the graph declaration and graph evaluation steps
- Can execute the code line by line while having full access to all variables
- PyTorch is funded and supported by Facebook

PyTorch Computation Graphs
---
- Its compuations based on a *directed acyclic graph* (DAG) (lol H5G throwback)
- Relies on building a computation graph at its core
    - Uses this graph to derive relationships between tensors from the input to the output
    - Simply a network of nodes where each node represents an operation

Create a simple graph representing z = 2 * (a - b) + c
```python
import torch
def compute_z(a, b, c):
    r1 = torch.sub(a, b)
    r2 = torch.mul(r1, 2)
    z = torch.add(r2, c)
    return z
```
- Now can simply cally this function with tensor objects as function arguments

Create a tensor to allow gradients to be computed:
```python3
a = torch.tensor(3.14, requires_grad=True)

# can also just use the requires_grad_() function
w = torch.tensor([1.0, 2.0, 3.0])
w.requires_grad_()

```
- As of 2021, only tensors of floating point and complex type can require gradients
- Set to false by default

- Initialzing model parameters with random weights is necessary to break the symmetry during backpropagation
    - Otherwise a multilayer NN would be no more useful than a single-layer NN like logistic regression
    - When creating a PyTorch tensor, can also use a random initialization scheme
- PyTorch can generate random numbers based aon a lot of probability distributions
```python3
# initialize using Glorot initialization
import torch.nn as nn

torch.manual_seed(1)
w.requires_grad_()
nn.init.xavier_normal_(w)
```
- It was often observed than random uniform or random normal weight initialization could result in poor model performance during training
- Now roughly balance the variance of the gradients across different layers
    - Otherwise some layers may get too much attention during training while others lag behind

Computing Gradients
---
- Optimizing NNs requires computing gradients of the loss wrt the NN weights
    - Required for algorithms like SGD
- Can also use gradients to find out why a NN model is making a particular prediction for a test sample
- PyTorch provides a context for calculating gradients of tensors wrt its dependent nodes in a computation graph
    - Can call the `backward` method from the `torch.autograd` module
    - Computes the sum of gradients of the given tenstor with regard to the terminal nodes in the graph

- Automatic Differentiation = like an implementation of the chain rule for computing gradients of nested functions
    - Set of computational techniques for computing gradients of arbitrary arithmetic operations
    - Gradients of a computation are obtained by accumulating the gradients through repeated applications of the chain rule
    - Uses reverse accumulation (dy/dx0 = dy/dx1 * dx1/dx0)

- Adversarial example = add some small imperceptible noise to the input which can result in misclassification


Simplifying Implementations of Common Architectures
---
- Use `nn.Sequential`
    - The layers stored inside the model are connected in a cascaded way
```python3
model = nn.Sequential(
    nn.Linear(4, 16),
    nn.ReLU(),
    nn.Linear(16, 32),
    nn.ReLU()
)
model

==>

Sequential(
    (0): Linear(in_features=4, out_features=16, bias=True)
    (1): ReLU()
    (2): Linear(in_features=16, out_features=32, bias=True)
    (3): ReLU()
)
```
- Specified the layers and instantiated the `model` after passing the layers to the `nn.Sequential` class
- Output of the first fully connected layer is the input to the first ReLU layer
- Output of the first ReLU layer is the input for the second fully connected layer
- Output of the second ReLU fully connected layer is the input to the second ReLU

- Can experiment with:
    - Choosing different activation functions
    - Initializing layer paramyers with `nn.init`
    - Applying regularization terms to the layer parameters to prevent overfitting (L1 or L2)

```
nn.init.xavier_uniform_(model[0].weight)
l1_weight = 0.01
l1_penalty = l1_weight * model[2].weight.abs().sum()
```
- Can specify which type of optimizer loss function you want
    - For optimizers, SGD and Adam are the most widely used methods
    - Choice depends on the task (ex. use MSE for a regression problem)
    - Can use precision, recall, accuracy, and AUC to evaluate models
```python3
loss_fn = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
```

- XOR classification problem analyzes capacity of a model with regard to capturing the nonlinear decision boundary between two classes
    - A simple model with no hidden layer can only derive a linear decision boundary ==> unable to solve XOR problem
    - Need to add one+ hidden layers connected via nonlinear activation functions
    - Thus can add a hidden layer and compare different numbers of hidden units until satisfactory results are achieved

- General rule of thumb: More layers, the more neurons, and the larger the capcity of the model will be
    - Capcity can be thought of how readuly the model can approximate complex functions
    - Larger models have more parameters but are usually harder to train (prone to overfitting)
    - Made network deeper than wider as fewer parameters are required to achieve a comparable model capacity
        - However this makes model prone to the vanishing / exploding gradient problem

Using nn.Module
---
- `Sequential` is good but it doesn't allow creating models that have multiple input, output, or intermediate branches
    - Thus use `nn.Module`

```python3
class MyModule(nn.Module):
    def __init__(self):
        super().__init__()
        l1 = nn.Linear(2, 4)
        a1 = nn.ReLU()
        l2 = nn.Linear(4, 4)
        a2 = nn.ReLU()
        l3 = nn.Linear(4, 1)
        a3 = nn.Sigmoid()
        l = [l1, a1, l2, a2, l3, a3]
        self.module_list = nn.ModuleList(l)

     def forward(self, x):
        for f in self.module_list:
            x = f(x)
        return x
```
- `forward()` method is used to specify the forward pass
- `__init__()` is used to define the layers as attrivutes of the class so they can be accessed via the `self` reference attribute
- Code above does the same thing as we were doing with `nn.Sequential`
- Put all the layers in the `nn.ModuleList` object which is just a list of `nn.Module` objects


Writing Custom Layers in PyTorch
---
- Can define a new layer that is not supported by PyTorch with a new class derived from `nn.Module`
- Imagine we want to define a new linear layer that computes w(x + E) + b , where E refers to a random variable as a noise variable

```python3
class NoisyLinear(nn.Module):
    def __init__(self, input_size, output_size, noise_stddev=0.1):
        super().__init__()
        w = torch.Tensor(input_size, output_size)
        self.w = nn.Parameter(w)  # nn.Parameter is a Tensor # that's a module parameter.
        
        nn.init.xavier_uniform_(self.w)
        b = torch.Tensor(output_size).fill_(0)
        self.b = nn.Parameter(b)
        self.noise_stddev = noise_stddev

    def forward(self, x, training=False):
        if training:
            noise = torch.normal(0.0, self.noise_stddev, x.shape)
            x_new = torch.add(x, noise)
        else:
            x_new = x
        return torch.add(torch.mm(x_new, self.w), self.b)
```
- Defined both constructor `__init__()` and `forward()` methods
    - In constructor, define variables and other required tensors for custom layer
    - Then can create variables and initialze them in the constructor if the `input_size` is given to the constructor
- `noise_stddev` is used to specify the std dev for the distribution of E (sampled from a gaussian distribution)
- Set `training=False` to distinguish whether the layer is used during training or for prediction (inference)

Their Example Notes
---
- Numeric data specifically refers to continuous data of the floating point type
- Two ways to work with a categorial feature
    - Using an embedding layer `nn.Embedding`
        - Maps each index to a vector of random numbers of the type float which can be trained
        - Think of it as a more effeicient implementation of a one-hot encoding multiplied with a trainable weight matrix
    - Using one-hot-encoded vectors (0 ==> [1, 0, 0], 1 ==> [0, 1, 0], etc.)
- When the number of categories is large, using the embedding layer with fewer dimensions than the number of categories can improve the performance


PyTorch Lightning
---
- Makes training deep neural networks simpler by removing much of the boilerplate code
- Has multi-GPU support and fast low-precision training
- There's a whole part about this in the textbook but I don't think I'm going to use it
